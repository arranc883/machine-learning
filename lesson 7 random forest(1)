#does not require standisation in data set
#overfit=does really well training but bad on test
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

data=pd.read_csv('C:\\Users\\Arran\\OneDrive\\Desktop\\machine learning\\data_sets\\student-mat.csv')

#print(data.info())
#print(data.describe())

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()

for i in data:
    if data[i].dtype == object:
        data[i]=le.fit_transform(data[i])
#independent
X=data.drop(['G1' , 'G2' , 'G3'], axis=1)
#dependent
y=data['G3']


print(X.head() , y.head())
from sklearn.model_selection import train_test_split
X_train , X_test , y_train , y_test = train_test_split(X , y , test_size=0.2 , random_state=5)

#averaging the results of 2 or more ml algo
from sklearn.ensemble import RandomForestClassifier
model=RandomForestClassifier(n_estimators=100)
model.fit(X_train , y_train)
y_predict=model.predict(X_test)

from sklearn.metrics import classification_report , confusion_matrix
import seaborn as sns
#classification report
print(classification_report(y_test , y_predict))

#cm
matrix=confusion_matrix(y_test , y_predict)
sns.heatmap(matrix,annot=True, fmt='d')
plt.title('confusion matrix')
plt.xlabel('independent')
plt.ylabel('dependent')
plt.show()



