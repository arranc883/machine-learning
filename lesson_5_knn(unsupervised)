import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


#pre processing
data=pd.read_csv('C:\\Users\\arran\\OneDrive\\Desktop\\machine learning\\lessons\\knn(unsuperviesed)\\iris.csv')

#unique values
print(data['species'].value_counts())

X=data.drop('species', axis=1)
y=data['species']

from sklearn.model_selection import train_test_split
X_train , X_test , y_train , y_test = train_test_split(X,y,test_size=0.2,random_state=5)


from sklearn.preprocessing import LabelEncoder , StandardScaler
#scales all of the data (so the knn doesnt think that a higher value column is more important than the others)
ss=StandardScaler()
ss.fit_transform(X_train)
ss.fit_transform(X_test)

#label encode (cat to num)
le=LabelEncoder()
le.fit_transform(y_train)
le.fit_transform(y_test)

#knn
from sklearn.neighbors import KNeighborsClassifier
model=KNeighborsClassifier(n_neighbors=5)

#training model
model.fit(X_train,y_train)

#predicting
pred=model.predict(X_test)

#analysis
from sklearn.metrics import classification_report , confusion_matrix
#classification report
print(classification_report(y_test , pred))

#confusion matrix
cm=confusion_matrix(y_test,pred)
#annot - anotations        fmt - decimal
sns.heatmap(cm,annot=True,fmt='d')
plt.title('confusion matrix')
plt.xlabel('predicted')
plt.ylabel('real data')
plt.show()
